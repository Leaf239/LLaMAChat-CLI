# ğŸ¦™ LLaMA 3.2 3B CLI Chat

This is a simple streaming chat interface using LLaMA 3.2 3B (GGUF) in the terminal.

## ğŸš€ Features

- Prompt history with role separation
- Streaming token-wise output
- Easy configuration

## ğŸ›  Installation

1. Clone this repository.
2. Download the GGUF model from here: [LLaMA 3.2 3B Instruct Q4_K_M](https://huggingface.co/TheBloke/Llama-3.2-3B-Instruct-GGUF)
3. Place the model in `./models` folder.
4. Run the installer:

```bash
python install.py
```

5. Launch the chat:

```bash
python main.py
```

## ğŸ§  Model info

- Model: LLaMA 3.2 3B Instruct
- Format: GGUF (Quantized: Q4_K_M)
- Context length: 4096 tokens
- Backend: llama-cpp-python

## ğŸ“‚ File Structure

```
llama_chat_cli/
â”œâ”€â”€ llama_chat/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ llm_engine.py
â”‚   â””â”€â”€ prompt_builder.py
â”œâ”€â”€ main.py
â”œâ”€â”€ install.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ Exit

Type `/exit` anytime to stop the chat.
